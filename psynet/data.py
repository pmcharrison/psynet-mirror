import csv
import io
import os
import tempfile
from typing import List, Optional
from zipfile import ZipFile

import dallinger.data
import dallinger.models
import pandas
import postgres_copy
import six
import sqlalchemy
from dallinger import db
from dallinger.data import fix_autoincrement
from dallinger.db import Base as SQLBase  # noqa
from dallinger.db import init_db  # noqa
from dallinger.experiment_server import dashboard
from dallinger.models import Info  # noqa
from dallinger.models import Network  # noqa
from dallinger.models import Node  # noqa
from dallinger.models import Notification  # noqa
from dallinger.models import Question  # noqa
from dallinger.models import Transformation  # noqa
from dallinger.models import Transmission  # noqa
from dallinger.models import Vector  # noqa
from dallinger.models import SharedMixin, timenow  # noqa
from joblib import Parallel, delayed
from progress.bar import Bar
from sqlalchemy import Column, String
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.schema import (
    DropConstraint,
    DropTable,
    ForeignKeyConstraint,
    MetaData,
    Table,
)

from . import field
from .field import VarStore
from .utils import classproperty


def export(class_name):
    """
    Export data from an experiment.

    Collects instance data for class_name, including inheriting models.
    """
    models = {}
    instances = db_models()[class_name].query.all()
    if len(instances) == 0:
        return models
    with Bar(f"Serializing {class_name} instances", max=len(instances)) as bar:
        for instance in instances:
            model = instance.__class__.__name__
            if model not in models:
                models[model] = []
            models[model].append(instance.__json__())
            bar.next()
    return models


class SQLMixinDallinger(SharedMixin):
    """
    We apply this Mixin class when subclassing Dallinger classes,
    for example ``Network`` and ``Info``.
    It adds a few useful exporting features,
    but most importantly it adds automatic mapping logic,
    so that polymorphic identities are constructed automatically from
    class names instead of having to be specified manually.
    For example:

    ```py
    from dallinger.models import Info

    class CustomInfo(Info)
        pass
    ```
    """

    __extra_vars__ = {}

    @property
    def var(self):
        return VarStore(self)

    def __json__(self):
        """
        Determines the information that is shown for this object in the dashboard
        and in the csv files generated by ``psynet export``.
        """
        x = {c: getattr(self, c) for c in self.sql_columns}

        field.json_clean(x, details=True)
        field.json_add_extra_vars(x, self)
        field.json_format_vars(x)

        return x

    @classproperty
    def sql_columns(cls):
        return cls.__mapper__.column_attrs.keys()

    @classproperty
    def parent_class(cls):
        return cls.__mro__[1]

    @classproperty
    def inherits_table(cls):
        return hasattr(cls.parent_class, "__table_name__")

    @declared_attr
    def __mapper_args__(cls):
        """
        This programmatic definition of polymorphic_identity and polymorphic_on
        means that users can define new SQLAlchemy classes without any reference
        to these SQLAlchemy constructs. Instead the polymorphic mappers are
        constructed automatically based on class names.
        """
        x = {"polymorphic_identity": cls.__name__}
        if not cls.inherits_table:
            x["polymorphic_on"] = cls.type
        return x


class SQLMixin(SQLMixinDallinger):
    """
    We apply this mixin when creating our own SQL-backed
    classes from scratch. For example:

    ```
    from psynet.data import SQLBase, SQLMixin, register_table

    @register_table
    class Bird(SQLBase, SQLMixin):
        __tablename__ = "bird"

    class Sparrow(Bird):
        pass
    ```
    """

    @declared_attr
    def type(cls):
        return Column(String(50))


def drop_all_db_tables(bind=db.engine):
    """
    Drops all tables from the Postgres database.
    Includes a workaround for the fact that SQLAlchemy doesn't provide a CASCADE option to ``drop_all``,
    which was causing errors with Dallinger's version of database resetting in ``init_db``.

    (https://github.com/pallets-eco/flask-sqlalchemy/issues/722)
    """
    engine = bind

    db.session.commit()

    con = engine.connect()
    trans = con.begin()
    inspector = sqlalchemy.inspect(engine)

    # We need to re-create a minimal metadata with only the required things to
    # successfully emit drop constraints and tables commands for postgres (based
    # on the actual schema of the running instance)
    meta = MetaData()
    tables = []
    all_fkeys = []

    for table_name in inspector.get_table_names():
        fkeys = []

        for fkey in inspector.get_foreign_keys(table_name):
            if not fkey["name"]:
                continue

            fkeys.append(ForeignKeyConstraint((), (), name=fkey["name"]))

        tables.append(Table(table_name, meta, *fkeys))
        all_fkeys.extend(fkeys)

    for fkey in all_fkeys:
        con.execute(DropConstraint(fkey))

    for table in tables:
        con.execute(DropTable(table))

    trans.commit()


dallinger.db.Base.metadata.drop_all = drop_all_db_tables


def dallinger_models():
    "A list of all base models in Dallinger"
    from .participant import Participant

    return {
        "Info": Info,
        "Network": Network,
        "Node": Node,
        "Notification": Notification,
        "Participant": Participant,
        "Question": Question,
        "Transformation": Transformation,
        "Transmission": Transmission,
        "Vector": Vector,
    }


# Extra base models that are defined in PsyNet or in the experiment itself
extra_models = {}


def db_models():
    "Together, this list of models should cover all the base classes in the database."
    return {
        **dallinger_models(),
        **extra_models,
    }


def register_table(cls):
    """
    This decorator should be applied whenever defining a new
    SQLAlchemy table.
    For example:

    ``` py
    @register_table
    class Bird(SQLBase, SQLMixin):
        __tablename__ = "bird"
    ```
    """
    extra_models[cls.__name__] = cls
    setattr(dallinger.models, cls.__name__, cls)
    update_dashboard_models()
    dallinger.data.table_names.append(cls.__tablename__)
    return cls


def update_dashboard_models():
    "Determines the list of objects in the dashboard database browser."
    from .timeline import Response
    from .trial.main import Trial

    dallinger.models.Trial = Trial
    dallinger.models.Response = Response

    dashboard.BROWSEABLE_MODELS = [
        "Participant",
        "Network",
        "Node",
        "Trial",
        "Response",
        "Transformation",
        "Transmission",
        "Notification",
    ] + list(extra_models)


def ingest_to_model(
    file,
    model,
    engine=None,
    clear_columns: Optional[List] = None,
    replace_columns: Optional[dict] = None,
):
    """
    Imports a CSV file to the database.

    Parameters
    ----------
    file :
        CSV file to import (specified as a file handler, created for example by open())

    model :
        SQLAlchemy class corresponding to the objects that should be created.

    clear_columns :
        Optional list of columns to clear when importing the CSV file.
        This is useful in the case of foreign-key constraints (e.g. participant IDs).

    replace_columns :
        Optional dictionary of values to set for particular columns.
    """
    # Patched version of dallinger.data.ingest_to_model
    if engine is None:
        engine = db.engine

    if clear_columns or replace_columns:
        with tempfile.TemporaryDirectory() as temp_dir:
            patched_csv = os.path.join(temp_dir, "patched.csv")
            patch_csv(file, patched_csv, clear_columns, replace_columns)
            with open(patched_csv, "r") as patched_csv_file:
                ingest_to_model(
                    patched_csv_file, model, clear_columns=None, replace_columns=None
                )
    else:
        inspector = sqlalchemy.inspect(db.engine)
        reader = csv.reader(file)
        columns = tuple('"{}"'.format(n) for n in next(reader))
        postgres_copy.copy_from(
            file, model, engine, columns=columns, format="csv", HEADER=False
        )
        if "id" in inspector.get_columns(model.__table__):
            fix_autoincrement(engine, model.__table__.name)


def patch_csv(infile, outfile, clear_columns, replace_columns):
    df = pandas.read_csv(infile)

    _replace_columns = {**{col: pandas.NA for col in clear_columns}, **replace_columns}

    for col, value in _replace_columns.items():
        df[col] = value

    df.to_csv(outfile, index=False)


def ingest_zip(path, engine=None):
    """
    Given a path to a zip file created with `export()`, recreate the
    database with the data stored in the included .csv files.
    This is a patched version of dallinger.data.ingest_zip that incorporates
    support for custom tables.
    """

    if engine is None:
        engine = db.engine

    inspector = sqlalchemy.inspect(engine)
    all_table_names = inspector.get_table_names()

    import_order = [
        "network",
        "participant",
        "node",
        "info",
        "notification",
        "question",
        "transformation",
        "vector",
        "transmission",
    ]

    for n in all_table_names:
        if n not in import_order:
            import_order.append(n)

    with ZipFile(path, "r") as archive:
        filenames = archive.namelist()

        for name in import_order:
            filename_template = f"data/{name}.csv"

            matches = [f for f in filenames if filename_template in f]
            if len(matches) == 0:
                continue
            elif len(matches) > 1:
                raise IOError(
                    f"Multiple matches for {filename_template} found in archive: {matches}"
                )
            else:
                filename = matches[0]

            model_name = name.capitalize()
            model = db_models()[model_name]
            file = archive.open(filename)
            if six.PY3:
                file = io.TextIOWrapper(file, encoding="utf8", newline="")
            ingest_to_model(file, model, engine)


dallinger.data.ingest_zip = ingest_zip
dallinger.data.ingest_to_model = ingest_to_model


def export_assets(path, n_parallel=8):
    # Assumes we already have loaded the experiment into the local database,
    # as would be the case if the function is called from psynet export.
    from .assets import Asset

    Parallel(n_jobs=n_parallel, verbose=10)(
        delayed(lambda a: a.export(root=path))(a) for a in Asset.query.all()
    )
