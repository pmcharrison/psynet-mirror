import dallinger.models
import sqlalchemy
from dallinger.db import Base as SQLBase  # noqa
from dallinger.db import init_db  # noqa
from dallinger.experiment_server import dashboard
from dallinger.models import Info  # noqa
from dallinger.models import Network  # noqa
from dallinger.models import Node  # noqa
from dallinger.models import Notification  # noqa
from dallinger.models import Question  # noqa
from dallinger.models import Transformation  # noqa
from dallinger.models import Transmission  # noqa
from dallinger.models import Vector  # noqa
from dallinger.models import SharedMixin, timenow  # noqa
from progress.bar import Bar
from sqlalchemy import Column, String
from sqlalchemy.schema import (
    DropConstraint,
    DropTable,
    ForeignKeyConstraint,
    MetaData,
    Table,
)

from .participant import Participant  # noqa
from .timeline import Response
from .trial.main import Trial
from .utils import classproperty


def export(class_name):
    """
    Export data from an experiment.

    Collects instance data for class_name, including inheriting models.
    """
    models = {}
    instances = db_models()[class_name].query.all()
    if len(instances) == 0:
        return models
    with Bar(f"Serializing {class_name} instances", max=len(instances)) as bar:
        for instance in instances:
            model = instance.__class__.__name__
            if model not in models:
                models[model] = []
            models[model].append(instance.__json__())
            bar.next()
    return models


class SQLMixin(SharedMixin):
    """
    This Mixin class is used to define custom SQLAlchemy objects. For example:

    ```py
    from psynet.data import SQLBase, SQLMixin, register_table

    @register_table
    class Bird(SQLBase, SQLMixin):
        __tablename__ = "bird"

    class Sparrow(Bird):
        pass
    ```
    """

    def __json__(self):
        """
        Determines the information that is shown for this object in the dashboard
        and in the csv files generated by ``psynet export``.
        """
        return {c: getattr(self, c) for c in self.sql_columns}

    @classproperty
    def sql_columns(cls):
        return cls.__mapper__.column_attrs.keys()

    @classproperty
    def parent_class(cls):
        return cls.__mro__[1]

    @classproperty
    def inherits_table(cls):
        return hasattr(cls.parent_class, "__table_name__")

    object_type = Column(String(50))

    @classproperty
    def __mapper_args__(cls):
        """
        This programmatic definition of polymorphic_identity and polymorphic_on
        means that users can define new SQLAlchemy classes without any reference
        to these SQLAlchemy constructs. Instead the polymorphic mappers are
        constructed automatically based on class names.
        """
        x = {"polymorphic_identity": cls.__name__}
        if not cls.inherits_table:
            x["polymorphic_on"] = cls.object_type
        return x


def drop_all_db_tables(bind):
    """
    Drops all tables from the Postgres database.
    Includes a workaround for the fact that SQLAlchemy doesn't provide a CASCADE option to ``drop_all``,
    which was causing errors with Dallinger's version of database resetting in ``init_db``.

    (https://github.com/pallets-eco/flask-sqlalchemy/issues/722)
    """
    engine = bind

    con = engine.connect()
    trans = con.begin()
    inspector = sqlalchemy.inspect(engine)

    # We need to re-create a minimal metadata with only the required things to
    # successfully emit drop constraints and tables commands for postgres (based
    # on the actual schema of the running instance)
    meta = MetaData()
    tables = []
    all_fkeys = []

    for table_name in inspector.get_table_names():
        fkeys = []

        for fkey in inspector.get_foreign_keys(table_name):
            if not fkey["name"]:
                continue

            fkeys.append(ForeignKeyConstraint((), (), name=fkey["name"]))

        tables.append(Table(table_name, meta, *fkeys))
        all_fkeys.extend(fkeys)

    for fkey in all_fkeys:
        con.execute(DropConstraint(fkey))

    for table in tables:
        con.execute(DropTable(table))

    trans.commit()


dallinger.db.Base.metadata.drop_all = drop_all_db_tables


def dallinger_models():
    "A list of all base models in Dallinger"
    return {
        "Info": Info,
        "Network": Network,
        "Node": Node,
        "Notification": Notification,
        "Participant": Participant,
        "Question": Question,
        "Transformation": Transformation,
        "Transmission": Transmission,
        "Vector": Vector,
    }


# Extra base models that are defined in PsyNet or in the experiment itself
extra_models = {}


def db_models():
    "Together, this list of models should cover all the base classes in the database."
    return {
        **dallinger_models(),
        **extra_models,
    }


def register_table(cls):
    """
    This decorator should be applied whenever defining a new
    SQLAlchemy table.
    For example:

    ``` py
    @register_table
    class Bird(SQLBase, SQLMixin):
        __tablename__ = "bird"
    ```
    """
    extra_models[cls.__name__] = cls
    setattr(dallinger.models, cls.__name__, cls)
    update_dashboard_models()
    return cls


def update_dashboard_models():
    "Determines the list of objects in the dashboard database browser."
    dallinger.models.Trial = Trial
    dallinger.models.Response = Response

    dashboard.BROWSEABLE_MODELS = [
        "Participant",
        "Network",
        "Node",
        "Trial",
        "Response",
        "Transformation",
        "Transmission",
        "Notification",
    ] + list(extra_models)
