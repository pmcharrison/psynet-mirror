# GANSpace experiment

This experiment gets participants to manipulate faces generated by a
Generative Adversarial Network (GAN).
In each trial, the participant manipulates a slider corresponding
to a principal component of the GAN's latent state.
These trials are combined together into Gibbs Sampling chains.

## Deploying

The experiment relies on an external server with a GPU for generating
the stimuli. The server is currently on AWS's EC2 service. When deploying
the experiment, you need to connect to this server to monitor the
stimulus generation process.

### Connecting to the GPU server

The first time you connect to the GPU server, you need to add your IP
address to the AWS firewall, so that you will be allowed to connect.
To do this, go to [Security Groups in AWS](https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#SecurityGroups:),
look for 'deep-learning-server', and modify the inbound rules. Add a rule
allowing all traffic from your current IP address.
You can get your public IP address by typing "What's my IP address" into Google.
Suppose it returns '86.8.141.1'; you should add '/32' to this and put it into the AWS console rule,
making for example '86.8.141.1/32'. This makes it a port range.
You will need to repeat this step if you connect to a different router,
for example if you take your office laptop home.

To connect to the server, you will also need a `pem` file, which constitutes
the server's access key. Ask Peter for this. Put this key somewhere safe on your computer.
Before you can use it, you need to change its permissions, by doing the following
in the terminal:

```
chmod 400 deep-learning-server.pem
```

where `deep-learning-server.pem` is the path to your `pem` file.

Now, you can connect to the server by running

```
ssh -i deep-learning-server.pem ubuntu@54.204.210.121
```

from a terminal, where `deep-learning-server.pem` is the path to your `pem` file.

### Monitoring the GPU server

The GANSpace API is hosted as a Flask app from `/home/ubuntu/ganspace`.
However, you should not need to change these files.
Instead, you mostly just need to monitor the app by running
`sudo journalctl -u ganspace -f` from the terminal.
This should display a live log from the server.
Each time the server receives a generation request, you should see something like:

```
May 19 17:49:04 ip-172-31-85-89 gunicorn[6637]: [2020-05-19 17:49:04 +0000] [6651] [INFO] Generating GANSpace video...
```

Every time it finishes, you should see something like:

```
May 19 17:49:31 ip-172-31-85-89 gunicorn[6637]: INFO:app:Video generated successfully. Uploading to S3...
```

Keep an eye out for error messages here.
You can close the log with `CTRL-C`.

It is sometimes also useful to check the GPU usage. You can see this by running

```
nvidia-smi
```

While synthesis is running, the usage should be generally high,
with dips when uploading to S3.

### Resetting the GPU server

When you run the GANSpace experiment, it queues up stimulus requests on the
GPU server, which are processed one at a time. Each request normally takes
about 30 seconds to complete: it involves generating an mp4 file corresponding to
a given slider, and uploading this mp4 file to S3.
Shutting the Dallinger experiment does not
clear the requests, by default, so if you're restarting the Dallinger experiment,
you must clear the request queue, so that the new experiment doesn't get blocked.
You can do this by running the following commands on the server:

```
sudo systemctl stop ganspace
sudo systemctl start ganspace
```

Check the logs to make sure the app restarted properly:

```
sudo journalctl -u ganspace -f
```

### Dallinger and stimulus generation

Dallinger makes a request to the GPU server each time a new node is created in the network.
This happens approximately every 5 trials. This triggers a call to `async_post_grow_network`,
which then makes an HTTP request to the GPU server. This HTTP request waits while
the stimulus is generated, and only finishes once the stimulus has been uploaded to S3.
If something goes wrong in this process, you will see an error in the Python log,
and typically an error also in the server log at `sudo journalctl -u ganspace -f`.

### What to watch out for

The main danger in this experiment is that recruitment is so fast that the GPU server can't keep up.
In this case, participants will finish the experiment early without completing the full number of trials.
You can see this behaviour by looking at the participant bonuses in the database;
low bonuses mean that the participant didn't complete many items.
Approximately speaking, since each stimulus is seen by 5 participants, each trial takes about 10 seconds,
and each stimulus takes 30 seconds to generate, the server can cope with 1-2 active participants
at a time, before it will start to run out of stimuli. Of course, this does not count participants
who are doing other parts of the experiment, and it does not count participants who have accepted the HIT
but have yet to begin it.

Incidentally, you can check whether a given network is waiting for stimulus generation by looking
at the monitor route, at `property5`: if it says `True`, that means the network is waiting.

### Final notes

To summarise, this experiment is deployed like a normal Dallinger experiment
(`dallinger deploy`), except you have one more thing to monitor: the GPU server.
Because of the time-consuming nature of stimulus generation, you need to be a bit careful
about participant recruitment, as the server cannot handle many participants at the same time.
However, it is not a disaster if you overrecruit, because the participants will simply exit
the experiment early.
